<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>path_panning.performance_measurement API documentation</title>
<meta name="description" content="Test the performance of the implemented algorithms â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>path_panning.performance_measurement</code></h1>
</header>
<section id="section-intro">
<p>Test the performance of the implemented algorithms.</p>
<p>Author: Marcel Schindhelm, Marian Friedrich</p>
<p>Copyright: German copyright law 2021, no publication, modification, Non-Commercial, No Derivative Works</p>
<p>Date created: 01.12.2021</p>
<p>Date last modified: 14.12.2021</p>
<p>Python Version: 3.9.7</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Test the performance of the implemented algorithms.

Author: Marcel Schindhelm, Marian Friedrich

Copyright: German copyright law 2021, no publication, modification, Non-Commercial, No Derivative Works

Date created: 01.12.2021

Date last modified: 14.12.2021

Python Version: 3.9.7
&#34;&#34;&#34;

import random
from statistics import mean, stdev
from time import perf_counter  # for performance measurement

# Algorithms
from a_star import AStarAlgo

from dijkstra import DijkstraAlgo

from floyd_warshall import FloydWarshallAlgo

# Read map files from file system
from map_parser import MapParser


class PerformanceMeasurement:
    &#34;&#34;&#34;Measure the performance of a given function.&#34;&#34;&#34;

    @staticmethod
    def measureExecutionTime(func, *args, **kwargs):
        &#34;&#34;&#34;Report the execution time of the function.

        Keyword arguments:
        func -- the function to be measured
        *args -- input arguments for the function
        **kwargs -- input arguments for the function

        Return: result of the function, execution time
        &#34;&#34;&#34;
        start = perf_counter()
        result = func(*args, **kwargs)
        end = perf_counter()
        deltaTime = ((end - start) * 1000000)
        return result, round(deltaTime, 1)


class TestAlgorithms:
    &#34;&#34;&#34;Test class for measuring the performance of all algorithms.&#34;&#34;&#34;

    def __init__(self, graphMap, seed=None):
        &#34;&#34;&#34;Initialize required variables.

        Keyword arguments:
        graphMap -- the map data structure for path planning
        seed -- the seed for determining start and end nodes
        &#34;&#34;&#34;
        self.aStarSolver = AStarAlgo()
        self.dijkstraSolver = DijkstraAlgo()
        self.warshallSolver = FloydWarshallAlgo()

        self.graphMap = graphMap
        self.seed = seed

        # Override random values
        self.startNodeOverride = -1
        self.endNodeOverride = -1

    def setNodeIdOverride(self, startNode, endNode):
        &#34;&#34;&#34;Override the start and end node.

        Keyword arguments:
        startNode -- the new start node
        endNode -- the new end node
        &#34;&#34;&#34;
        self.startNodeOverride = startNode
        self.endNodeOverride = endNode

    def getRandomNodeIDs(self):
        &#34;&#34;&#34;Return random ids if they arent overwritten.

        Note: If the node override values aren&#39;t set to negative one
        it will always return startNodeOverride, endNodeOverride

        Return startNodeID, endNodeID
        &#34;&#34;&#34;
        if self.startNodeOverride != -1 and self.endNodeOverride != -1:
            return self.startNodeOverride, self.endNodeOverride

        random.seed(self.seed)  # used for generating nodes

        # Determine start and end node
        startNodeIndex = random.randint(0, len(self.graphMap.nodePositionDict) - 1)
        endNodeIndex = random.randint(0, len(self.graphMap.nodePositionDict) - 1)

        # Get the node ids
        startNodeId = self.graphMap.nodePositionDict[startNodeIndex].id
        endNodeId = self.graphMap.nodePositionDict[endNodeIndex].id

        return startNodeId, endNodeId

    def testPathfindingPerformance(self, repetitions=100, multiplePaths=False):
        &#34;&#34;&#34;Run the performance test for the algorithms.

        Keyword arguments:
        repetitions -- the number of runs for the test
        multiplePaths -- if true, the test will choose random paths for each run
        &#34;&#34;&#34;
        # Perform Measurement
        aStarTimeList = []
        aDijkstraTimeList = []
        aWarshallTimeList = []

        if not multiplePaths:
            startNode, endNode = self.getRandomNodeIDs()

        for i in range(1, repetitions + 1):
            if multiplePaths:
                startNode, endNode = self.getRandomNodeIDs()

            # Run A Star, measure performance
            _, elapsedTime = PerformanceMeasurement.measureExecutionTime(
                self.aStarSolver.solveAlgoritm,
                startNode, endNode, self.graphMap)
            aStarTimeList.append(elapsedTime)

            # Run Dijkstra, measure performance
            _, elapsedTime = PerformanceMeasurement.measureExecutionTime(
                self.dijkstraSolver.solveAlgoritm,
                startNode, endNode, self.graphMap)
            aDijkstraTimeList.append(elapsedTime)

            # Run Floyd Warshall, measure performance
            _, elapsedTime = PerformanceMeasurement.measureExecutionTime(
                self.warshallSolver.solveAlgoritm, startNode, endNode, self.graphMap)
            aWarshallTimeList.append(elapsedTime)

            # Print update each 1000 iterations
            if (i % 1000 == 0 and i != 0) or i == repetitions:
                print(f&#34;&#34;&#34;Processed {i} of {repetitions}...&#34;&#34;&#34;)

        # Print results to the console
        print(f&#34;&#34;&#34;Timing results in micro seconds with {repetitions} repetitions:&#34;&#34;&#34;)
        if not multiplePaths:
            print(f&#34;&#34;&#34;Start node: {startNode} --&gt; End node: {endNode}&#34;&#34;&#34;)
        print(f&#34;&#34;&#34;AStar results, Average: {mean(aStarTimeList)} , Std. Dev: {stdev(aStarTimeList)}
            - min:{min(aStarTimeList)}, max: {max(aStarTimeList)}&#34;&#34;&#34;)
        print(f&#34;&#34;&#34;Dijkstra results, Average: {mean(aDijkstraTimeList)} , Std. Dev: {stdev(aDijkstraTimeList)}
            - min:{min(aDijkstraTimeList)}, max: {max(aDijkstraTimeList)}&#34;&#34;&#34;)
        print(f&#34;&#34;&#34;Warshall results, Average: {mean(aWarshallTimeList)} , Std. Dev: {stdev(aWarshallTimeList)}
            - min:{min(aWarshallTimeList)}, max: {max(aWarshallTimeList)}, first run: {aWarshallTimeList[0]}&#34;&#34;&#34;)
        aWarshallTimeList.pop(0)
        print(f&#34;&#34;&#34;Warshall results without 1st, Average: {mean(aWarshallTimeList)} , Std. Dev: {stdev(aWarshallTimeList)}
            - min:{min(aWarshallTimeList)}, max: {max(aWarshallTimeList)}&#34;&#34;&#34;)


def main():
    &#34;&#34;&#34;Measure the runtime of the implemented algorithms.&#34;&#34;&#34;
    # Parse default map file
    mapParser = MapParser()
    filepath = mapParser.GetDefaultMapFilepath(&#39;modellstadt&#39;)
    graphMap = mapParser.ParseMapData(filepath)

    # Test the algorithms with the parsed map (optionally set a random seed)
    testAlgos = TestAlgorithms(graphMap, seed=None)
    # testAlgos.setNodeIdOverride(0, 7) # override random values, so all test will run between start and end
    testAlgos.testPathfindingPerformance(multiplePaths=False, repetitions=10000)


if __name__ == &#39;__main__&#39;:
    main()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="path_panning.performance_measurement.main"><code class="name flex">
<span>def <span class="ident">main</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Measure the runtime of the implemented algorithms.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def main():
    &#34;&#34;&#34;Measure the runtime of the implemented algorithms.&#34;&#34;&#34;
    # Parse default map file
    mapParser = MapParser()
    filepath = mapParser.GetDefaultMapFilepath(&#39;modellstadt&#39;)
    graphMap = mapParser.ParseMapData(filepath)

    # Test the algorithms with the parsed map (optionally set a random seed)
    testAlgos = TestAlgorithms(graphMap, seed=None)
    # testAlgos.setNodeIdOverride(0, 7) # override random values, so all test will run between start and end
    testAlgos.testPathfindingPerformance(multiplePaths=False, repetitions=10000)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="path_panning.performance_measurement.PerformanceMeasurement"><code class="flex name class">
<span>class <span class="ident">PerformanceMeasurement</span></span>
</code></dt>
<dd>
<div class="desc"><p>Measure the performance of a given function.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PerformanceMeasurement:
    &#34;&#34;&#34;Measure the performance of a given function.&#34;&#34;&#34;

    @staticmethod
    def measureExecutionTime(func, *args, **kwargs):
        &#34;&#34;&#34;Report the execution time of the function.

        Keyword arguments:
        func -- the function to be measured
        *args -- input arguments for the function
        **kwargs -- input arguments for the function

        Return: result of the function, execution time
        &#34;&#34;&#34;
        start = perf_counter()
        result = func(*args, **kwargs)
        end = perf_counter()
        deltaTime = ((end - start) * 1000000)
        return result, round(deltaTime, 1)</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="path_panning.performance_measurement.PerformanceMeasurement.measureExecutionTime"><code class="name flex">
<span>def <span class="ident">measureExecutionTime</span></span>(<span>func, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Report the execution time of the function.</p>
<p>Keyword arguments:
func &ndash; the function to be measured
<em>args &ndash; input arguments for the function
</em>*kwargs &ndash; input arguments for the function</p>
<p>Return: result of the function, execution time</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def measureExecutionTime(func, *args, **kwargs):
    &#34;&#34;&#34;Report the execution time of the function.

    Keyword arguments:
    func -- the function to be measured
    *args -- input arguments for the function
    **kwargs -- input arguments for the function

    Return: result of the function, execution time
    &#34;&#34;&#34;
    start = perf_counter()
    result = func(*args, **kwargs)
    end = perf_counter()
    deltaTime = ((end - start) * 1000000)
    return result, round(deltaTime, 1)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="path_panning.performance_measurement.TestAlgorithms"><code class="flex name class">
<span>class <span class="ident">TestAlgorithms</span></span>
<span>(</span><span>graphMap, seed=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Test class for measuring the performance of all algorithms.</p>
<p>Initialize required variables.</p>
<p>Keyword arguments:
graphMap &ndash; the map data structure for path planning
seed &ndash; the seed for determining start and end nodes</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TestAlgorithms:
    &#34;&#34;&#34;Test class for measuring the performance of all algorithms.&#34;&#34;&#34;

    def __init__(self, graphMap, seed=None):
        &#34;&#34;&#34;Initialize required variables.

        Keyword arguments:
        graphMap -- the map data structure for path planning
        seed -- the seed for determining start and end nodes
        &#34;&#34;&#34;
        self.aStarSolver = AStarAlgo()
        self.dijkstraSolver = DijkstraAlgo()
        self.warshallSolver = FloydWarshallAlgo()

        self.graphMap = graphMap
        self.seed = seed

        # Override random values
        self.startNodeOverride = -1
        self.endNodeOverride = -1

    def setNodeIdOverride(self, startNode, endNode):
        &#34;&#34;&#34;Override the start and end node.

        Keyword arguments:
        startNode -- the new start node
        endNode -- the new end node
        &#34;&#34;&#34;
        self.startNodeOverride = startNode
        self.endNodeOverride = endNode

    def getRandomNodeIDs(self):
        &#34;&#34;&#34;Return random ids if they arent overwritten.

        Note: If the node override values aren&#39;t set to negative one
        it will always return startNodeOverride, endNodeOverride

        Return startNodeID, endNodeID
        &#34;&#34;&#34;
        if self.startNodeOverride != -1 and self.endNodeOverride != -1:
            return self.startNodeOverride, self.endNodeOverride

        random.seed(self.seed)  # used for generating nodes

        # Determine start and end node
        startNodeIndex = random.randint(0, len(self.graphMap.nodePositionDict) - 1)
        endNodeIndex = random.randint(0, len(self.graphMap.nodePositionDict) - 1)

        # Get the node ids
        startNodeId = self.graphMap.nodePositionDict[startNodeIndex].id
        endNodeId = self.graphMap.nodePositionDict[endNodeIndex].id

        return startNodeId, endNodeId

    def testPathfindingPerformance(self, repetitions=100, multiplePaths=False):
        &#34;&#34;&#34;Run the performance test for the algorithms.

        Keyword arguments:
        repetitions -- the number of runs for the test
        multiplePaths -- if true, the test will choose random paths for each run
        &#34;&#34;&#34;
        # Perform Measurement
        aStarTimeList = []
        aDijkstraTimeList = []
        aWarshallTimeList = []

        if not multiplePaths:
            startNode, endNode = self.getRandomNodeIDs()

        for i in range(1, repetitions + 1):
            if multiplePaths:
                startNode, endNode = self.getRandomNodeIDs()

            # Run A Star, measure performance
            _, elapsedTime = PerformanceMeasurement.measureExecutionTime(
                self.aStarSolver.solveAlgoritm,
                startNode, endNode, self.graphMap)
            aStarTimeList.append(elapsedTime)

            # Run Dijkstra, measure performance
            _, elapsedTime = PerformanceMeasurement.measureExecutionTime(
                self.dijkstraSolver.solveAlgoritm,
                startNode, endNode, self.graphMap)
            aDijkstraTimeList.append(elapsedTime)

            # Run Floyd Warshall, measure performance
            _, elapsedTime = PerformanceMeasurement.measureExecutionTime(
                self.warshallSolver.solveAlgoritm, startNode, endNode, self.graphMap)
            aWarshallTimeList.append(elapsedTime)

            # Print update each 1000 iterations
            if (i % 1000 == 0 and i != 0) or i == repetitions:
                print(f&#34;&#34;&#34;Processed {i} of {repetitions}...&#34;&#34;&#34;)

        # Print results to the console
        print(f&#34;&#34;&#34;Timing results in micro seconds with {repetitions} repetitions:&#34;&#34;&#34;)
        if not multiplePaths:
            print(f&#34;&#34;&#34;Start node: {startNode} --&gt; End node: {endNode}&#34;&#34;&#34;)
        print(f&#34;&#34;&#34;AStar results, Average: {mean(aStarTimeList)} , Std. Dev: {stdev(aStarTimeList)}
            - min:{min(aStarTimeList)}, max: {max(aStarTimeList)}&#34;&#34;&#34;)
        print(f&#34;&#34;&#34;Dijkstra results, Average: {mean(aDijkstraTimeList)} , Std. Dev: {stdev(aDijkstraTimeList)}
            - min:{min(aDijkstraTimeList)}, max: {max(aDijkstraTimeList)}&#34;&#34;&#34;)
        print(f&#34;&#34;&#34;Warshall results, Average: {mean(aWarshallTimeList)} , Std. Dev: {stdev(aWarshallTimeList)}
            - min:{min(aWarshallTimeList)}, max: {max(aWarshallTimeList)}, first run: {aWarshallTimeList[0]}&#34;&#34;&#34;)
        aWarshallTimeList.pop(0)
        print(f&#34;&#34;&#34;Warshall results without 1st, Average: {mean(aWarshallTimeList)} , Std. Dev: {stdev(aWarshallTimeList)}
            - min:{min(aWarshallTimeList)}, max: {max(aWarshallTimeList)}&#34;&#34;&#34;)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="path_panning.performance_measurement.TestAlgorithms.getRandomNodeIDs"><code class="name flex">
<span>def <span class="ident">getRandomNodeIDs</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return random ids if they arent overwritten.</p>
<p>Note: If the node override values aren't set to negative one
it will always return startNodeOverride, endNodeOverride</p>
<p>Return startNodeID, endNodeID</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def getRandomNodeIDs(self):
    &#34;&#34;&#34;Return random ids if they arent overwritten.

    Note: If the node override values aren&#39;t set to negative one
    it will always return startNodeOverride, endNodeOverride

    Return startNodeID, endNodeID
    &#34;&#34;&#34;
    if self.startNodeOverride != -1 and self.endNodeOverride != -1:
        return self.startNodeOverride, self.endNodeOverride

    random.seed(self.seed)  # used for generating nodes

    # Determine start and end node
    startNodeIndex = random.randint(0, len(self.graphMap.nodePositionDict) - 1)
    endNodeIndex = random.randint(0, len(self.graphMap.nodePositionDict) - 1)

    # Get the node ids
    startNodeId = self.graphMap.nodePositionDict[startNodeIndex].id
    endNodeId = self.graphMap.nodePositionDict[endNodeIndex].id

    return startNodeId, endNodeId</code></pre>
</details>
</dd>
<dt id="path_panning.performance_measurement.TestAlgorithms.setNodeIdOverride"><code class="name flex">
<span>def <span class="ident">setNodeIdOverride</span></span>(<span>self, startNode, endNode)</span>
</code></dt>
<dd>
<div class="desc"><p>Override the start and end node.</p>
<p>Keyword arguments:
startNode &ndash; the new start node
endNode &ndash; the new end node</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setNodeIdOverride(self, startNode, endNode):
    &#34;&#34;&#34;Override the start and end node.

    Keyword arguments:
    startNode -- the new start node
    endNode -- the new end node
    &#34;&#34;&#34;
    self.startNodeOverride = startNode
    self.endNodeOverride = endNode</code></pre>
</details>
</dd>
<dt id="path_panning.performance_measurement.TestAlgorithms.testPathfindingPerformance"><code class="name flex">
<span>def <span class="ident">testPathfindingPerformance</span></span>(<span>self, repetitions=100, multiplePaths=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Run the performance test for the algorithms.</p>
<p>Keyword arguments:
repetitions &ndash; the number of runs for the test
multiplePaths &ndash; if true, the test will choose random paths for each run</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def testPathfindingPerformance(self, repetitions=100, multiplePaths=False):
    &#34;&#34;&#34;Run the performance test for the algorithms.

    Keyword arguments:
    repetitions -- the number of runs for the test
    multiplePaths -- if true, the test will choose random paths for each run
    &#34;&#34;&#34;
    # Perform Measurement
    aStarTimeList = []
    aDijkstraTimeList = []
    aWarshallTimeList = []

    if not multiplePaths:
        startNode, endNode = self.getRandomNodeIDs()

    for i in range(1, repetitions + 1):
        if multiplePaths:
            startNode, endNode = self.getRandomNodeIDs()

        # Run A Star, measure performance
        _, elapsedTime = PerformanceMeasurement.measureExecutionTime(
            self.aStarSolver.solveAlgoritm,
            startNode, endNode, self.graphMap)
        aStarTimeList.append(elapsedTime)

        # Run Dijkstra, measure performance
        _, elapsedTime = PerformanceMeasurement.measureExecutionTime(
            self.dijkstraSolver.solveAlgoritm,
            startNode, endNode, self.graphMap)
        aDijkstraTimeList.append(elapsedTime)

        # Run Floyd Warshall, measure performance
        _, elapsedTime = PerformanceMeasurement.measureExecutionTime(
            self.warshallSolver.solveAlgoritm, startNode, endNode, self.graphMap)
        aWarshallTimeList.append(elapsedTime)

        # Print update each 1000 iterations
        if (i % 1000 == 0 and i != 0) or i == repetitions:
            print(f&#34;&#34;&#34;Processed {i} of {repetitions}...&#34;&#34;&#34;)

    # Print results to the console
    print(f&#34;&#34;&#34;Timing results in micro seconds with {repetitions} repetitions:&#34;&#34;&#34;)
    if not multiplePaths:
        print(f&#34;&#34;&#34;Start node: {startNode} --&gt; End node: {endNode}&#34;&#34;&#34;)
    print(f&#34;&#34;&#34;AStar results, Average: {mean(aStarTimeList)} , Std. Dev: {stdev(aStarTimeList)}
        - min:{min(aStarTimeList)}, max: {max(aStarTimeList)}&#34;&#34;&#34;)
    print(f&#34;&#34;&#34;Dijkstra results, Average: {mean(aDijkstraTimeList)} , Std. Dev: {stdev(aDijkstraTimeList)}
        - min:{min(aDijkstraTimeList)}, max: {max(aDijkstraTimeList)}&#34;&#34;&#34;)
    print(f&#34;&#34;&#34;Warshall results, Average: {mean(aWarshallTimeList)} , Std. Dev: {stdev(aWarshallTimeList)}
        - min:{min(aWarshallTimeList)}, max: {max(aWarshallTimeList)}, first run: {aWarshallTimeList[0]}&#34;&#34;&#34;)
    aWarshallTimeList.pop(0)
    print(f&#34;&#34;&#34;Warshall results without 1st, Average: {mean(aWarshallTimeList)} , Std. Dev: {stdev(aWarshallTimeList)}
        - min:{min(aWarshallTimeList)}, max: {max(aWarshallTimeList)}&#34;&#34;&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="path_panning" href="index.html">path_panning</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="path_panning.performance_measurement.main" href="#path_panning.performance_measurement.main">main</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="path_panning.performance_measurement.PerformanceMeasurement" href="#path_panning.performance_measurement.PerformanceMeasurement">PerformanceMeasurement</a></code></h4>
<ul class="">
<li><code><a title="path_panning.performance_measurement.PerformanceMeasurement.measureExecutionTime" href="#path_panning.performance_measurement.PerformanceMeasurement.measureExecutionTime">measureExecutionTime</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="path_panning.performance_measurement.TestAlgorithms" href="#path_panning.performance_measurement.TestAlgorithms">TestAlgorithms</a></code></h4>
<ul class="">
<li><code><a title="path_panning.performance_measurement.TestAlgorithms.getRandomNodeIDs" href="#path_panning.performance_measurement.TestAlgorithms.getRandomNodeIDs">getRandomNodeIDs</a></code></li>
<li><code><a title="path_panning.performance_measurement.TestAlgorithms.setNodeIdOverride" href="#path_panning.performance_measurement.TestAlgorithms.setNodeIdOverride">setNodeIdOverride</a></code></li>
<li><code><a title="path_panning.performance_measurement.TestAlgorithms.testPathfindingPerformance" href="#path_panning.performance_measurement.TestAlgorithms.testPathfindingPerformance">testPathfindingPerformance</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>